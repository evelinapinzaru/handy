# Handy - Computer Vision-Based Gesture Control for Presentations

## Overview
Handy is a mobile application that enables hands-free control of desktop presentations using computer vision and hand gesture recognition.\
\
**Why mobile?** Presentation laptops often face away from the presenter, making reliable camera-based gesture tracking difficult. A phone can be positioned conveniently to solve this issue.

## Features
- âœ‹ Swipe right/left to navigate slides
- ğŸ“± Works with any Android phone
- ğŸ’» Compatible with major presentation software (PowerPoint, Keynote, PDF viewers, Google Slides)
- ğŸ”„ Low latency via real-time WebSocket communication

## Tech Stack (demo version)

### Core Technologies
- **Computer Vision**: MediaPipe, OpenCV
- **Mobile Framework**: Kivy (cross-platform)
- **Communication**: WebSockets
- **Desktop Control**: PyAutoGUI

### Supporting Libraries
- **NumPy**: Mathematical operations & array processing
- **Buildozer**: Android APK packaging (dev tool)

## Project Structure
```
handy/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ \_\_init\_\_.py
â”‚   â”œâ”€â”€ detector.py       # Gesture detection logic
â”‚   â”œâ”€â”€ server.py         # Laptop WebSocket server
â”‚   â””â”€â”€ client.py         # Phone app
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ \_\_init\_\_.py
â”‚   â””â”€â”€ test\_camera.py   # Test hand tracking
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py       # Configuration (IP, thresholds, ports)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ buildozer.spec        # Android build configuration
â””â”€â”€ README.md
```

## How It Works
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Phone     â”‚         â”‚   Laptop     â”‚         â”‚ Presentationâ”‚
â”‚             â”‚         â”‚              â”‚         â”‚  Software   â”‚
â”‚  MediaPipe  â”‚ â”€â”€â”€â”€â”€â–¶  â”‚  WebSocket   â”‚ â”€â”€â”€â”€â”€â–¶  â”‚  (PowerPointâ”‚
â”‚   Camera    â”‚ Gesture â”‚   Server     â”‚ Keyboardâ”‚   /PDF)     â”‚
â”‚             â”‚  JSON   â”‚              â”‚  Events â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Phone**: Captures video, detects hand landmarks, recognizes gestures
2. **Communication**: Sends gesture commands via WebSocket
3. **Laptop**: Receives commands, simulates keyboard presses
4. **Result**: Presentation advances/reverses

## Roadmap

### â³  Tier 1 - MVP (In progress)
- Hand detection with MediaPipe
- Swipe left/right gestures
- WebSocket communication
- Basic gesture smoothing

### ğŸš§ Tier 2 - Demo-Ready (Planned)
- Laser pointer (finger tracking)
- Point gesture recognition
- Visual feedback on phone
- Connection status indicator

### ğŸ“‹ Tier 3 - Production Features (Planned)
- Multi-gesture support (5-7 gestures)
- Kalman filtering for smoother tracking
- Customizable sensitivity settings
- Distance-based speed control